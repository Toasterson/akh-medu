Neuro-Symbolic Architectures for Modular Knowledge Transfer and Hardware-Efficient Inference
The progression of artificial intelligence toward generalizable and efficient systems has necessitated a rigorous re-evaluation of how knowledge is represented, stored, and transferred between distinct models. Historically, the field has been polarized between symbolic systems, which rely on explicit rules and human-readable logic, and sub-symbolic neural networks, which excel at pattern recognition through data-driven learning.1 As the computational demands of large language models (LLMs) continue to escalate, the requirement for architectures that can execute on traditional central processing units (CPUs) while managing massive parameter sets via external storage has become a primary engineering objective. Central to this objective is the selection of an optimal symbolic language—whether phonetic, purely symbolic, or ideographic—and the implementation of modular frameworks, such as skillpacks, that facilitate the dynamic loading and transfer of specialized capabilities.
The Taxonomy of Representational Modalities in Synthetic Intelligence
The choice of a symbolic language for artificial intelligence involves a trade-off between semantic density, cross-lingual transferability, and neural grounding. Representational systems can be broadly categorized into phonetic-mappable systems, purely symbolic systems based on formal logic, and ideographic metalanguages.
Phonetic and Mappable Representations
Phonetic representations, specifically those based on phonological scripts (PhonoS), operate through the assembly of sounds to form meaning.3 In human cognition, reading phonological scripts such as English or Korean Hangul involves converting graphemes into phonemes, a process characterized as "assembled phonology".3 For an AI model, a phonetic symbolic language is "mappable" because it provides a language-agnostic input that can bridge the performance gap between high-resource and low-resource languages.4
The utility of phonetic representation in neural architectures lies in its ability to capture the intrinsic manifold structure of speech and language data. Research indicates that speech data represented in high-dimensional spaces lie on low-dimensional manifolds that can be used to map phonetic information to coordinate spaces.5 By utilizing phonetic features that are less dependent on specific orthographic systems, models can reduce linguistic gaps and improve cross-lingual alignment.4 However, neural language models often struggle to capture phonology implicitly from orthography alone, as this information is limited by the encoding and embedding strategies employed during training.6
Purely Symbolic Systems and Formal Logic
Purely symbolic artificial intelligence, often referred to as classical or logic-based AI (GOFAI), utilizes high-level human-readable representations such as logic programming, production rules, and semantic nets.7 These systems are built upon the manipulation of discrete symbols—words, numbers, or abstract concepts—using clearly defined logical rules.8 The primary strength of this paradigm is its transparency and interpretability; every decision can be traced back to a specific rule, making it ideal for safety-critical applications such as aviation or financial compliance.1
The computational density of purely symbolic systems is relatively low compared to neural models, as they require explicit hand-crafted rules to handle every possible scenario. This results in the "knowledge acquisition bottleneck," where the system fails to generalize beyond its predefined logic and struggles with the ambiguity of unstructured data.2 Despite these limitations, symbolic AI remains essential for "System 2" thinking—slow, deliberate, and algorithmic reasoning that complements the fast, intuitive pattern recognition of neural "System 1" thinking.9
Ideographic and Logographic Metalanguages
Ideographic representations, or logograms, represent semantic components directly as words or morphemes, bypassing the sound-to-symbol assembly required by phonetic systems.11 In these systems, phonology is "addressed" by retrieving it from an internal lexicon associated with the logographic character.3 An ideographic metalanguage for AI can serve as a universal communication framework that transcends cultural and linguistic boundaries.12
The Neuro-symbolic Ideographic Metalanguage (NIM) approach utilizes the theory of Natural Semantic Metalanguage (NSM) to achieve the semantic decomposition of complex ideas into simpler, atomic concepts.12 This allows a model to augment its large-scale parametric knowledge with structured ontologies, guiding the reasoning process through visual ideographs that are inherently language-agnostic.12 Ideographic scripts also retain a "rich store of inclusive perception" and gestalt-like properties that analytical phonetic alphabets may erode.13
Representation Type
	Primary Mechanism
	Neural Activation Path
	Core Advantage
	Main Limitation
	Phonetic (PhonoS)
	Assembled Phonology
	Left Dorsal Pathway (AG/SMG)
	Cross-lingual Alignment
	High Assembly Overhead
	Logographic (LogoS)
	Addressed Phonology
	Internal Lexicon Retrieval
	Semantic Universality
	High Lexical Volume
	Purely Symbolic
	Formal Logic
	Rule-based Inference
	Absolute Transparency
	Inflexibility
	Ideographic
	Semantic Decomposition
	Gestalt Perception
	Cognitive Intuition
	Syntactic Complexity
	Architectural Requirements for CPU-Centric Inference
Executing large-scale AI models on CPUs requires addressing the von Neumann bottleneck—the performance mismatch between high-speed processors and slower memory modules.14 Traditional CPUs optimize for latency tolerance and flexible control flow, whereas AI workloads demand high arithmetic intensity.15 To overcome these constraints, models must leverage quantization, bit-level sparsity, and specialized memory management techniques.
Quantization and Integer-Only Computation
Model quantization is a critical enabler for deploying deep neural networks (DNNs) on CPU-based platforms. By replacing high-precision floating-point values (e.g., FP32) with low-precision integers (e.g., INT8, INT4), the memory footprint and computational overhead are significantly reduced.14 Integer-only quantization, as demonstrated by the I-BERT architecture, enables the deployment of Transformer models on hardware without floating-point units, achieving up to a 3.5x latency speedup.16
Effective quantization for CPU deployment requires customized strategies that consider model mapping schemes and quantization granularity.14 While dense layers are often sensitive to quantization, embeddings can frequently be compressed to INT4 or even INT2 without significant accuracy loss.17 This reduction in precision allows for higher arithmetic intensity, as the processor can execute more operations per byte of data moved from memory.15
Sparsity and Structured Pruning
Deep neural networks trained with sparsity-inducing regularization can achieve weight sparsity between 20% and 80%.15 In a sparse model, a significant portion of the weights are zero and can be skipped during computation. Structured sparsity patterns, such as block-level or row-column pruning, are particularly effective for CPUs because they maintain regular memory access patterns while reducing the total number of multiply-accumulate (MAC) operations.15
Sparsity is particularly relevant in Mixture-of-Experts (MoE) architectures, where only a small subset of the model's total parameters is activated for each input token.18 This sparse activation allows the model to scale to hundreds of billions of parameters while maintaining a low computational cost during the forward pass.19
Memory Management and Demand Paging
To run models that exceed the physical RAM capacity of a CPU, systems must utilize memory mapping (mmap) and demand paging. The mmap protocol allows a file on disk to be mapped into the application's virtual memory address space, enabling the OS to treat disk storage as if it were part of the main memory.20 Specific segments of the weight file are loaded into RAM only when they are accessed, which minimizes the peak memory usage during model initialization.20
However, the reliance on mmap introduces challenges such as "compute jitter" caused by OS scheduling overhead or page faults.23 If the OS does not efficiently prefetch weights from storage, the processor may idle while waiting for data, increasing latency.14 Advanced optimization strategies, such as using eBPF programs to hook into kernel events, can be used to guide swapping decisions and improve the predictability of inference latency.23


Strategy
	Mechanism
	CPU Benefit
	RAM Impact
	Quantization
	INT8/INT4 precision
	Higher throughput
	2x-4x reduction
	Bit-level Sparsity
	Skipping zero-weights
	Reduced FLOPs
	Variable reduction
	mmap (Memory Map)
	Virtual address mapping
	Low startup latency
	Near-zero peak spike 21
	Sequential Loading
	Parameter-by-parameter copy
	Avoids memory spikes
	Minimal resident set 22
	Disk-to-RAM Orchestration and Expert Offloading
The practical execution of modular AI requires a sophisticated orchestration layer that can manage the movement of weights between high-speed RAM and secondary storage (SSDs). This is especially important for MoE models, where the total parameter count can be significantly higher than the capacity of consumer-grade memory.
FlashMoE and ML-Based Caching
FlashMoE represents a system-level framework designed for efficient MoE inference under limited RAM.19 The architecture separates experts from non-experts (such as attention modules and routing gates), storing the massive expert layers on an SSD.19 Because the non-expert layers constitute only 5-7% of the total model, they are kept resident in memory, while experts are subdivided and loaded on demand.19
The primary bottleneck in this approach is the I/O latency of the SSD. To mitigate this, FlashMoE incorporates a lightweight machine learning-based caching strategy that combines recency and frequency signals to maximize expert reuse.19 By approximating Belady's optimal policy, this system can achieve a cache hit rate up to 51% higher than traditional heuristic policies like LRU (Least Recently Used) or LFU (Least Frequently Used).19
High-Bandwidth Storage Interfaces
The performance of on-demand weight loading is heavily influenced by the bandwidth of the storage interface. PCIe 5.0 SSDs deliver a theoretical bandwidth of up to 16 GB/s, doubling the speed of PCIe 4.0.24 While this increased bandwidth is ideal for rapid data access, the actual loading times are often limited by the CPU's ability to decompress data on the fly and manage the memory pipeline.26


Interface
	Max Speed (x4)
	10GB Load Time (Theoretical)
	10GB Load Time (Real World)
	PCIe 3.0
	~4 GB/s
	2.5s
	3.0s - 3.5s
	PCIe 4.0
	~8 GB/s
	1.25s
	1.4s - 1.6s
	PCIe 5.0
	~16 GB/s
	0.625s
	0.7s - 0.9s 25
	Despite the raw speed of PCIe 5.0, RAM remains approximately 100 times faster in terms of access latency.26 Consequently, efficient disk offloading requires "pre-gated" mechanisms where the router predicts which experts will be needed in future layers, allowing the system to overlap I/O with computation.19
Modular Skillpacks and Knowledge Fusion
The concept of "skillpacks" refers to a modular architecture where specific capabilities are encapsulated into transferable neural components. This paradigm is primarily realized through the integration of Mixture-of-Experts (MoE) and Low-Rank Adaptation (LoRA).
MoE-LoRA Framework
In a MoE-LoRA architecture, a large pre-trained model serves as a frozen backbone, while multiple parallel low-rank expert adapters are injected into the layers.18 Each expert is a compact module representing a learned update $\Delta W = BA$, where $r \ll \min(d, k)$.18 These experts can be independently specialized for tasks such as medical question-answering, code generation, or legal analysis.29
This design allows for "on-the-fly specialization" by swapping in new skillpacks without retraining the entire model.30 The routing mechanism, coordinated via a symbolic identifier, assigns input-dependent weights to these experts, enabling adaptive multi-task learning.28 The use of "Knowledge-Preservation Plugins" further isolates general-purpose reasoning from specialized expert knowledge, preventing catastrophic forgetting during domain adaptation.29
Skill Path and Circuit Graphs
Advanced research into knowledge fusion involves unveiling the "skill path" within a language model through circuit graphs.31 By analyzing the internal chains and "echoes" of a prompt, researchers can identify the functional circuits (e.g., AND, OR, and ADDER gates) that constitute a specific language skill.31 This allows for the systematic modularization of LLMs, where skills are not just weights but structured directed acyclic graphs (DAGs) of functional modules.31
The "SkillPacks" paradigm, as presented in recent ICLR 2026 proceedings, focuses on the fusion of these modular units across different models.31 This enables a "plug-and-play" capability where a skill learned by one model can be extracted as a skillpack and integrated into another model's architecture, provided there is a common symbolic interface for the router.30
Symbolic Linking and Dual-Indexing Architectures
For a model to effectively "identify links" between skillpacks and modular knowledge, it must employ a neuro-symbolic dual-indexing architecture. This framework reconciles the semantic flexibility of neural embeddings with the structured reasoning of symbolic graphs.34
Sparse Graph Skeletons and Multi-Hop Traversal
A dual-indexing system consists of a sparse graph skeleton constructed from high-centrality document chunks and a complementary bipartite keyword index for broad semantic coverage.34 Retrieval is formulated as a constrained optimization problem, utilizing algorithms such as Prize-Collecting Steiner Trees (PCST) for subgraph extraction and Personalized PageRank (PPR) for multi-hop traversal.34
This architecture allows a 7-billion parameter model to match the performance of much larger models, such as GPT-4, on complex multi-hop question-answering tasks.34 By using a single-step graph-guided inference instead of iterative retrieval, the system can provide contextualized representations to the generator with sub-second latency.34
Knowledge Graphs as Semantic Models
In this neuro-symbolic context, knowledge graphs serve as the data model for the skillpacks. Every entity and relation within the graph must have a label, creating a flexible schema that can adapt to different business needs.33 Relations expressed as subject-label-object triplets can carry metadata and symbolic pointers, allowing the AI to ground its understanding in structured facts.35
When a model encounters a query, it performs "entity linking" to identify seed nodes in the graph.34 These seed nodes then trigger the activation of corresponding skillpacks or document chunks. This process is often refined through multi-stage extraction, where specialized generators identify cities, events, or organizations independently before merging the results into a consistent graph representation.33
Cognitive and Evolutionary Foundations of Symbolism
The development of symbolic languages for AI is deeply rooted in cognitive science and the philosophy of psychology. The "Symbolic Species" hypothesis suggests that the defining feature of human intelligence is the ability to link arbitrary symbols to abstract meanings, repurposing older neural circuits for symbolic thought.37
Massive Modularity and Context Sensitivity
The mind is hypothesized to be "massively modular," with cognitive functionalities realized by modules of varying sizes and formats.38 While some modules handle domain-specific abilities like perception or language parsing, higher cognitive processes are context-sensitive and flexible.38 This mirrors the AI challenge of balancing modular "skillpacks" (local computations) with a central reasoning process that can coordinate these modules.38
In AI systems, this is reflected in the distinction between System 1 and System 2.9 Neural networks excel at System 1 thinking (fast, intuitive pattern recognition), while symbolic logic handles System 2 thinking (slow, deliberative planning and deduction).9 Neuro-symbolic agents act as the bridge between these systems, interpreting user goals flexibly through neural models while planning and making decisions deterministically through symbolic logic.36
The Frame Problem and Ontological Promiscuity
A significant challenge in symbolic knowledge representation is the "frame problem"—the difficulty of specifying what does not change when an action takes place in a dynamic environment.39 For instance, if a robot picks up a cube from a table, a logical system must be explicitly told that the table does not disappear.39
To handle diverse knowledge types within a single framework, researchers have proposed the principle of "ontological promiscuity," where several ontologies are mixed to cover range of everyday items, temporal rules, and physical laws.39 This modularity in knowledge engineering allows for sharing and querying knowledge bases across different agents, provided they adhere to the same conceptual vocabulary or ontology.39
Future Directions in Modular AI and Hardware Synthesis
The future of AI lies in the convergence of high-level symbolic programming and low-level neural optimization. "Neurosymbolic programming" emerges as a field that bridges these areas, where functions are represented as programs that can use neural modules as primitives.10 These programs are induced using a combination of symbolic search and gradient-based optimization, offering advantages in interpretability, formal verification, and data efficiency.10
Computing-in-Memory and Analog Accelerators
As the limits of standard CPU architectures are reached, specialized hardware like Computing-in-Memory (CIM) and analog accelerators (ACIM) may become more prevalent. CIM systems integrate computing and memory units to eliminate data movement, though they often require model quantization to low-precision integers to handle the resolution constraints of analog-to-digital converters.14 These hardware advances will further enable the deployment of large-scale MoE models on edge devices by minimizing the energy and latency costs of weight retrieval.
Autonomous Agents and Knowledge Integration
The embodiment of neuro-symbolic AI in autonomous agents represents a shift from static models to goal-driven entities.36 These agents can collaborate in multi-agent systems (MAS), mirroring a team of specialized experts.36 The integration of "Knowledge Fusion via Modular SkillPacks" will allow these agents to dynamically acquire and share new skills, scaling intelligence across enterprise-scale distributed systems.31
In conclusion, the development of a symbolic language for AI that is both phonetic-mappable (for cross-lingual transfer and assembly) and ideographic (for semantic anchoring and universal communication) provides the necessary foundation for modular knowledge transfer. By optimizing CPU-centric execution through quantization, sparsity, and memory mapping, and by managing skillpacks through dual-indexing and SSD offloading, the AI industry can build systems that are transparent, efficient, and capable of complex reasoning beyond simple pattern recognition.
Comparative Evaluation of Retrieval Paradigms
The efficiency of retrieving and linking skillpacks is demonstrated by the performance of different indexing strategies. While exhaustive knowledge graph construction is computationally expensive, sparse skeleton construction based on eigenvector centrality provides a cost-effective alternative.


Metric
	Exhaustive Knowledge Graph
	Neural-Symbolic Dual-Indexing
	Construction Cost
	10.0x
	1.0x (Baseline) 34
	Retrieval Latency
	> 1.0s
	< 0.2s
	Retrieval Coverage
	75.0%
	92.4%
	Generation Quality Improvement
	10.0%
	32.4%
	Multi-hop Reasoning Ability
	High
	High (matches GPT-4) 34
	This data indicates that the synergistic integration of graph structures and vector embeddings (neural-symbolic coupling) is the most viable path for scaling enterprise-level retrieval and modular intelligence. The ability to identify hubs of semantic connectivity allows the system to prioritize entity extraction and skill mapping, ensuring that the model can navigate complex knowledge domains with minimal computational overhead.
Works cited
1. What are the trade-offs between symbolic and neural reasoning? - Milvus, accessed on February 7, 2026, https://milvus.io/ai-quick-reference/what-are-the-tradeoffs-between-symbolic-and-neural-reasoning
2. Symbolic AI vs Machine learning in NLP - Altamira, accessed on February 7, 2026, https://www.altamira.ai/blog/symbolic-ai-vs-machine-learning-in-nlp/
3. Human Brain Mapping of Visual Script Familiarity between Phonological and Logographic Language: 3 T Functional MRI Study - PMC, accessed on February 7, 2026, https://pmc.ncbi.nlm.nih.gov/articles/PMC5516734/
4. arXiv:2402.14279v3 [cs.CL] 15 Nov 2024, accessed on February 7, 2026, https://arxiv.org/pdf/2402.14279
5. Computing low-dimensional representations of speech from socio-auditory structures for phonetic analyses - PMC, accessed on February 7, 2026, https://pmc.ncbi.nlm.nih.gov/articles/PMC6706093/
6. Phonetic Representation for Language Modeling with Rhyme, accessed on February 7, 2026, https://swabhs.com/fall23-csci499-lm4nlp/assets/reports/JaivDoshi_NathanSparks_DanielYoun%20(1).pdf
7. Symbolic artificial intelligence - Wikipedia, accessed on February 7, 2026, https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence
8. Symbolic AI vs. Sub-symbolic AI: Understanding the Key Differences - SmythOS, accessed on February 7, 2026, https://smythos.com/developers/agent-development/symbolic-ai-vs-sub-symbolic-ai/
9. Neuro-symbolic AI - Wikipedia, accessed on February 7, 2026, https://en.wikipedia.org/wiki/Neuro-symbolic_AI
10. Neurosymbolic Programming - UT Austin Computer Science, accessed on February 7, 2026, https://www.cs.utexas.edu/~swarat/pubs/PGL-049-Plain.pdf
11. Logogram - Wikipedia, accessed on February 7, 2026, https://en.wikipedia.org/wiki/Logogram
12. NIM: Neuro-symbolic Ideographic Metalanguage for Inclusive Communication - arXiv, accessed on February 7, 2026, https://arxiv.org/html/2510.10459v1
13. About Marshall McLuhan and the differences between a phonetic and ideographic language : r/linguistics - Reddit, accessed on February 7, 2026, https://www.reddit.com/r/linguistics/comments/8oktfm/about_marshall_mcluhan_and_the_differences/
14. Model quantization for computing-in-memory: a survey, accessed on February 7, 2026, http://scis.scichina.com/en/2025/211401.pdf
15. Architectural Divergence in AI Accelerators: Beyond the Memory Wall and Systolic Arrays | by Nagesh Vishnumurthy | Jan, 2026 | Medium, accessed on February 7, 2026, https://medium.com/@kvnagesh/architectural-divergence-in-ai-accelerators-beyond-the-memory-wall-and-systolic-arrays-de19dd2d973f
16. Full Stack Approach for Efficient Deep Learning Inference - UC Berkeley, accessed on February 7, 2026, https://escholarship.org/content/qt4wf834q8/qt4wf834q8.pdf
17. Building Highly Efficient Inference System for Recommenders Using PyTorch, accessed on February 7, 2026, https://pytorch.org/blog/building-highly-efficient-inference-system-for-recommenders-using-pytorch/
18. MoE LoRA: Efficient Low-Rank Adaptation in DNNs - Emergent Mind, accessed on February 7, 2026, https://www.emergentmind.com/topics/mixture-of-experts-low-rank-adaptation-moe-lora
19. FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices - arXiv, accessed on February 7, 2026, https://arxiv.org/html/2601.17063v1
20. Loading big models into memory - Hugging Face, accessed on February 7, 2026, https://huggingface.co/docs/accelerate/concept_guides/big_model_inference
21. LLMs-from-scratch/ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict.ipynb at main - GitHub, accessed on February 7, 2026, https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict.ipynb
22. Memory-Efficient Model Weight Loading in PyTorch - Analytics Vidhya, accessed on February 7, 2026, https://www.analyticsvidhya.com/blog/2024/10/memory-efficient-model-weight-loading-in-pytorch/
23. OS-Level Challenges in LLM Inference and Optimizations - eunomia-bpf, accessed on February 7, 2026, https://eunomia.dev/blog/2025/02/18/os-level-challenges-in-llm-inference-and-optimizations/
24. PCIe 5.0 SSD: A Comprehensive Guide - Lenovo, accessed on February 7, 2026, https://www.lenovo.com/us/en/knowledgebase/pcie-50-ssd-a-comprehensive-guide/
25. What is PCIe 5.0?Is PCIe 5.0 Worth It? A Deep Dive into Speed and Performance, accessed on February 7, 2026, https://www.silicon-power.com/knowledge-detail/what-is-pcie5/
26. I tested Star Citizen loading times with PCIe 4.0 vs. PCIe 5.0 SSDs, so you dont have to do it. : r/starcitizen - Reddit, accessed on February 7, 2026, https://www.reddit.com/r/starcitizen/comments/1nmus3l/i_tested_star_citizen_loading_times_with_pcie_40/
27. PCIe 4.0 vs. PCIe 5.0: Which Should You Buy? - Kingspec Tech, accessed on February 7, 2026, https://www.kingspectech.com/blogs/posts/pcie-4-0-vs-5-0
28. LoRA-Based Modular Experts - Emergent Mind, accessed on February 7, 2026, https://www.emergentmind.com/topics/lora-based-modular-experts
29. [2601.07935] Towards Specialized Generalists: A Multi-Task MoE-LoRA Framework for Domain-Specific LLM Adaptation - arXiv, accessed on February 7, 2026, https://arxiv.org/abs/2601.07935
30. Hot Swapping AI Skills: Attention Mechanisms & ALoRA Explained - YouTube, accessed on February 7, 2026, https://www.youtube.com/watch?v=qmUWsFCnsz4
31. Wenya WANG (NTU) - Nanyang Technological University, accessed on February 7, 2026, https://personal.ntu.edu.sg/wangwy/
32. Publications - Jing Li, accessed on February 7, 2026, https://www.li-jing.com/paperlist.html
33. Inside SynaLinks: How Knowledge Graphs Power Neuro-Symbolic AI - Memgraph, accessed on February 7, 2026, https://memgraph.com/blog/deep-learning-knowledge-graph
34. (PDF) Neural-Symbolic Dual-Indexing Architectures for Scalable ..., accessed on February 7, 2026, https://www.researchgate.net/publication/398849995_Neural-Symbolic_Dual-Indexing_Architectures_for_Scalable_Retrieval-Augmented_Generation
35. Neuro-symbolic AI: The key to truly intelligent systems - metaphacts Blog, accessed on February 7, 2026, https://blog.metaphacts.com/neuro-symbolic-ai-the-key-to-truly-intelligent-systems
36. From Logic to Learning: The Future of AI Lies in Neuro-Symbolic Agents, accessed on February 7, 2026, https://builder.aws.com/content/2uYUowZxjkh80uc0s2bUji0C9FP/from-logic-to-learning-the-future-of-ai-lies-in-neuro-symbolic-agents
37. The Architecture of Language in the Human Brain | by Riaz Laghari | Medium, accessed on February 7, 2026, https://medium.com/@riazleghari/the-architecture-of-language-in-the-human-brain-91ffabb5871d
38. Review of Fodor, Jerry (2000). The Mind Doesn't Work That Way: The Scope and Limits of Computational Psychology - ResearchGate, accessed on February 7, 2026, https://www.researchgate.net/publication/241730120_Review_of_Fodor_Jerry_2000_The_Mind_Doesn't_Work_That_Way_The_Scope_and_Limits_of_Computational_Psychology
39. 12.3: Computational Knowledge Representation - Social Sci LibreTexts, accessed on February 7, 2026, https://socialsci.libretexts.org/Bookshelves/Psychology/Cognitive_Psychology/Cognitive_Psychology_and_Cognitive_Neuroscience_(Wikibooks)/12%3A_Knowledge_Representation_and_Hemispheric_Specialization/12.03%3A_Computational_Knowledge_Representation